{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lerrlera/IAT360-CV-Project-Group14/blob/myBranch/CV_Sign_For_Help.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Computer Vision Problem: Sign For Help\n",
        "###Group 14\n",
        "Our computer vision project aims to develop a model capable of detecting and recognizing the [Signal for Help (SFH)](https://canadianwomen.org/signal-for-help/) hand gestureâ€“a universal distress signal used by individuals experiencing violence or abuse. The problem we are addressing is the lack of accessible and automated tools that can identify this silent call for help, which could support early intervention and improve personal safety.\n",
        "\n",
        "This is an important issue because many victims cannot ask for help out loud (e.g., the abuser is nearby), and this automatic gesture recognition could enable faster responses and potentially save lives. Our model could be used in real life to detect SFH through smartphone cameras, wearable devices, telehealth sessions, or public service kiosks, enabling faster intervention and support for individuals experiencing abuse.\n"
      ],
      "metadata": {
        "id": "Bv14wo_2zaAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Google Drive & Libraries"
      ],
      "metadata": {
        "id": "gDFT0qKI0CvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paDGoDTG1lVG",
        "outputId": "bdcc5833-e623-41c2-a711-322afa46dfa8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "GRvr2Agf0Jc1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Description\n",
        "For this project, we need image data resembling two specific hand gestures as a picture below:\n",
        "\n",
        "*   An open hand with tucked thumb\n",
        "*   A closed hand with a trapped thumb\n",
        "\n",
        "\n",
        "\n",
        "![Banner_EN_CWF-1024x467.png](data:image/png;base64,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)\n",
        "\n",
        "**Source:** Signal for help | Use Signal to ask for help.  (n.d.). Canadian Womenâ€™s Foundation. https://canadianwomen.org/signal-for-help/\n",
        "\n",
        "###Details\n",
        "\n",
        "These images were sourced from 2 existing datasets and organized into 2 class folders, â€˜open_handâ€™ and â€˜closed_handâ€™ for a classification task. Specifically, we used images labelled â€˜Bâ€™ from the ASL dataset from the ASL dataset for our â€˜open_handâ€™ class, and images labelled â€˜Aâ€™ and â€˜Sâ€™ for our â€˜closed_handâ€™ class. We also supplemented our data with additional images from the Hands Images dataset, which better capture the specific thumb positions we needed and include hands wearing gloves.\n",
        "\n",
        "We have 2 labels, â€˜open_handâ€™ and â€˜closed_handâ€™. Since this is a supervised classification task, the folder structure itself serves as a label; images in the â€˜open_handâ€™ folder belong to â€˜open_handâ€™ class, and the same labelling rule applies for those in the â€˜closed_handâ€™ folder.\n",
        "\n",
        "###Adding More Data\n",
        "We also added 30 (15 for each class) new images each sourced from stock images (Adobe), our own photos of hands...\n",
        "\n",
        "\n",
        "###Data Split\n",
        "\n",
        "Total number of images in each class was: 3,052. 2/3 of this data should was used as training data, the rest is for evaluation:\n",
        "\n",
        "**â€˜open_handâ€™**\n",
        "*   2,035 images in train folder\n",
        "*   1,017 images in test folder\n",
        "\n",
        "**â€˜closed_handâ€™**\n",
        "*   2,035 images in train folder\n",
        "*   1,017 images in test folder\n",
        "\n",
        "\n",
        "Here is a Google Drive link to the zipped datasets used in this project:\n",
        "\n",
        "\n",
        "\n",
        "*   [SFH Dataset](https://drive.google.com/drive/folders/1puxrFn5ijZAyBCCPxtzP0QTTBX_QTSmd?usp=sharing)\n",
        "\n",
        "\n",
        "Here are existing datasets we used to collect this data:\n",
        "\n",
        "*   [ASL Alphabet Dataset on Kaggle](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)\n",
        "*   [Hands Images Dataset on Hugging Face](https://huggingface.co/datasets/trashsock/hands-images)"
      ],
      "metadata": {
        "id": "T8_ORqc10TxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "For preprocessing:\n",
        "- We have to convert the dataset into that acceptable by YOLO.\n",
        "  - Heirarchy of folders should be the one accepted by YOLO\n",
        "  - Labels format should be darknet YOLO.\n",
        "  - Labels files should be .txt files\n"
      ],
      "metadata": {
        "id": "7aQjNM0hDaBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heirarchy of Folders Correction"
      ],
      "metadata": {
        "id": "YOsIYgK-Po-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining paths to train and test folders."
      ],
      "metadata": {
        "id": "a06DXoMuKt97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original codes below: https://github.com/jaejams/Week3-ClassicML/blob/main/week_3_machinelearning.ipynb"
      ],
      "metadata": {
        "id": "InZ_BYKav8Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the required libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import soundfile\n",
        "import os\n",
        "# matplotlib complains about the behaviour of librosa.display, so we'll ignore those warnings:\n",
        "import warnings; warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "c9JZS55mwcEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "\n",
        "def load_data():\n",
        "    X,y=[],[]\n",
        "    count = 0\n",
        "    for file in glob.glob(\"Audio Data/*/*.wav\"):\n",
        "        file_name=os.path.basename(file)\n",
        "        features = get_features(file) #extract features\n",
        "        X.append(features) #extract features using get_features function #store as x, y variables\n",
        "        count += 1\n",
        "        # '\\r' + end='' results in printing over same line\n",
        "        print('\\r' + f' Processed {count}/{1440} audio samples',end=' ')\n",
        "    # Return arrays to plug into sklearn's cross-validation algorithms\n",
        "    return np.array(X)"
      ],
      "metadata": {
        "id": "l-jcco9ssk7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#these are the same codes, but we are just using different data.\n",
        "\n",
        "\n",
        "############# Unscaled test/validation set #############\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    features, #feature set\n",
        "    emotions, #label set\n",
        "    test_size=0.2,\n",
        "    random_state=42 #random_state - everytime you run this code, it will give you the same split. If you change it, it will give you a different split. actually shuffle data based on random number.\n",
        ")\n"
      ],
      "metadata": {
        "id": "ptw5w3cJs4rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original file for below codes: https://github.com/jaejams/Computer-Vision-Project/blob/main/ComputerVisionProject-revised.ipynb"
      ],
      "metadata": {
        "id": "alH3iygLtcE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = '/content/drive/MyDrive/Tutorials/Datasets/FRUIT DATA/test_zip/test'\n",
        "train_dir = '/content/drive/MyDrive/Tutorials/Datasets/FRUIT DATA/train_zip/train'"
      ],
      "metadata": {
        "id": "aZ3K__xXBGXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#run it once for making directories\n",
        "\n",
        "#os.makedirs('/content/drive/MyDrive/Tutorials/Datasets/FRUIT DATA/Final_data')\n",
        "os.makedirs('/content/Datasets/FRUIT DATA/Final_data/images')\n",
        "os.makedirs('/content/Datasets/FRUIT DATA/Final_data/labels')\n",
        "os.makedirs('/content/Datasets/FRUIT DATA/Final_data/images/train')\n",
        "os.makedirs('/content/Datasets/FRUIT DATA/Final_data/images/val')\n",
        "os.makedirs('/content/Datasets/FRUIT DATA/Final_data/labels/train')\n",
        "os.makedirs('/content/Datasets/FRUIT DATA/Final_data/labels/val')\n"
      ],
      "metadata": {
        "id": "6XEu3Hf6tgTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set the paths to labels and images directory\n",
        "label_dir= \"/content/Datasets/FRUIT DATA/Final_data/labels\"\n",
        "image_dir=\"/content/Datasets/FRUIT DATA/Final_data/images\""
      ],
      "metadata": {
        "id": "fTk2CptItkHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import shutil\n",
        "#run it once for copying!\n",
        "for file in os.listdir(train_dir):\n",
        "    if file.endswith(\".xml\"):\n",
        "        shutil.copy(os.path.join(train_dir, file), os.path.join(label_dir, \"train\"))\n",
        "    if file.endswith(\".jpg\"):\n",
        "        image = Image.open(os.path.join(train_dir, file))\n",
        "        image = image.convert(\"RGB\")\n",
        "        new_filename = os.path.splitext(file)[0] + \".jpg\"\n",
        "        image.save(os.path.join(image_dir,\"train\", new_filename), \"JPEG\")\n",
        "        #shutil.copy(os.path.join(train_dir, file), os.path.join(image_dir, \"train\"))\n",
        "\n",
        "\n",
        "#Copy XML files and jpg files from the test folder to the folders created.\n",
        "for file in os.listdir(test_dir):\n",
        "    if file.endswith(\".xml\"):\n",
        "        shutil.copy(os.path.join(test_dir, file), os.path.join(label_dir, \"val\"))\n",
        "    if file.endswith(\".jpg\"):\n",
        "        image = Image.open(os.path.join(test_dir, file))\n",
        "        image = image.convert(\"RGB\")\n",
        "        new_filename = os.path.splitext(file)[0] + \".jpg\"\n",
        "        image.save(os.path.join(image_dir,\"val\", new_filename), \"JPEG\")\n",
        "        #shutil.copy(os.path.join(test_dir, file), os.path.join(image_dir, \"val\"))\n"
      ],
      "metadata": {
        "id": "j2sGpYnxtp3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize first four sample images from train data\n",
        "for idx, image in enumerate(os.listdir(os.path.join(image_dir, \"train\"))):\n",
        "    img = cv2.imread(os.path.join(image_dir,\"train\", image), 1)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "    if idx == 3:\n",
        "        break"
      ],
      "metadata": {
        "id": "w2VxtWu-tvtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom gesture classification using YOLO\n",
        "\n",
        "YOLOv8 classification models use the -cls suffix, i.e. yolov8n-cls.pt and are pretrained on ImageNet. See Classification Docs for full details.\n",
        "\n",
        "Original file: https://github.com/jaejams/Computer-Vision-Project-Overview/blob/main/copy_of_yolov8_tutorial.ipynb\n",
        "https://github.com/jaejams/Computer-Vision-Project/blob/main/ComputerVisionProject-revised.ipynb"
      ],
      "metadata": {
        "id": "dIX6amSFuijN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "yZC4JREnu9SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8n-cls, train it on mnist160 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-cls.pt')  # load a pretrained YOLOv8n classification model\n"
      ],
      "metadata": {
        "id": "GhqL2GXyu17t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "DKmJc-tGvCVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(data='mnist160', epochs=3)  # train the model"
      ],
      "metadata": {
        "id": "Z2S9pjMPvDFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yaml\n",
        "path: /content/Datasets/FRUIT DATA/Final_data\n",
        "train: /content/Datasets/FRUIT DATA/Final_data/images/train\n",
        "#test: (test dataset folder path)\n",
        "val: /content/Datasets/FRUIT DATA/Final_data/images/val\n",
        "\n",
        "# Classes\n",
        "nc: 3 # replace based on your dataset's number of classes\n",
        "\n",
        "# Class names\n",
        "# replace all class names with your own classes' names\n",
        "names:\n",
        "  0: orange\n",
        "  1: apple\n",
        "  2: banana"
      ],
      "metadata": {
        "id": "iSuvRqF1vGT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(data=\"config.yaml\",epochs=5,patience=5,batch=8, lr0=0.0005,imgsz=640)"
      ],
      "metadata": {
        "id": "VS2RnADLvKhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
        "metrics.box.map    # map50-95\n",
        "metrics.box.map50  # map50\n",
        "metrics.box.map75  # map75\n",
        "metrics.box.maps   # a list contains map50-95 of each category\n",
        "metrics.box.mp    # P\n",
        "metrics.box.mr    # R"
      ],
      "metadata": {
        "id": "gXSz9AC9vPhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "\n",
        "Image.open('/content/runs/detect/train/confusion_matrix_normalized.png')"
      ],
      "metadata": {
        "id": "MU8yLuWZvQxd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}